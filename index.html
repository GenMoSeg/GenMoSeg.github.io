<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos">
  <meta property="og:title" content="MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos">
  <meta property="og:description" content="MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos">
  <meta property="og:image" content="https://shape-of-motion.github.io/static/images/open_graph.png">
  <meta property="twitter:title" content="MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos">
  <meta property="twitter:description" content="MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos">
  <meta property="twitter:image" content="https://shape-of-motion.github.io/static/images/open_graph.png">
  <meta property="og:type" content="website">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="4D reconstruction, Dynamic Reconstruction, 3D Tracking, Long-Range Motion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- <title>MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos</title> -->
  <title>Generalizable Motion Segmentation with 3D Spatial-Temporal Information Priori-Guided</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="./static/js/lqm.js"></script> -->
  <script src="./static/js/video_comparison.js" defer></script>

  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽ¬</text></svg>">

</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h1 class="title is-1 publication-title">Generalizable Motion Segmentation with 3D Spatial-Temporal Information Priori-Guided</h1>
            <!-- <h3 class="title is-1 publication-title">Anonymous Authors</h3> -->
            <!-- <h1 class="title is-4 publication-conference"> ICLR 2025 </h1> -->
            <div class="is-size-4 publication-authors">

<!--                <span class="author-block">
                Anonymous Authors</span> -->
              <span class="author-block">
                <h6>
                  <a href="" target="_blank">Shuo Zhang</a><sup>1</sup>,
                  <!-- <a href="https://liuyuan-pal.github.io/" target="_blank">Yuan Liu</a><sup>2*</sup>,
                  <a href="https://jiepengwang.github.io/" target="_blank">Jiepeng Wang</a><sup>2</sup>,
                  <a href="" target="_blank">Xianqiang Lyv</a><sup>1</sup>,<br>
                  <a href="https://quartz-khaan-c6f.notion.site/Peng-Wang-0ab0a2521ecf40f5836581770c14219c" target="_blank">Peng Wang</a><sup>2</sup>,
                  <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank">Wenping Wang</a><sup>3</sup>,
                  <a href="" target="_blank">Junhui Hou</a><sup>1&dagger;</sup> -->
                </h6>
              </span>

            </div>
            <div class="is-size-5 publication-authors">
              <p>
                <sup>1</sup>Wuhan University &nbsp;&nbsp;
                <!-- <sup>2</sup>The University of Hong Kong&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>3</sup>Texas A&M University&nbsp;&nbsp;&nbsp;&nbsp; -->
                <br>
                <!-- <sup>*</sup> Contribute equally. &nbsp;&nbsp; -->
                <sup>&dagger;</sup>Corresponding author. &nbsp;&nbsp;
              </p>
            </div>

              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="container is-max-desktop">
    <h2 style="text-align: center;" class="is-size-5">Zero-shot cross domain motion segmentation with 3D spatio-temporal information priori-guided, </h2>
    <h2 style="text-align: center;" class="is-size-5">Enhanced 4D renconstruction pipeline</h2>
    <img src="static/fig_contrib.png" alt="Image description" style="display: block; margin: auto;">
    <br><br>
    <h2 class="title is-3">Qulitative Results of Motion Segmentation</h2>
    <img src="static/fig_seg_show.png" alt="Image description" style="display: block; margin: auto;">
  </div>




  <!-- <section class="section">
    <div class="container is-max-desktop">
      <video id="teaser" width="100%" playsinline  autoplay loop muted>
        <source src="static/videos/teaser.mp4" type="video/mp4" />
      </video>
      <script>
        document.getElementById('teaser').play();
      </script>
    </div>
  </section> -->

  <!-- resutl on DyNeRF dataset -->
  <!-- resutl on DyNeRF dataset -->
  <!-- resutl on DyNeRF dataset -->
  <!-- resutl on DyNeRF dataset -->
  <section>
    <!-- </div> -->

    <div class="container is-max-desktop">
      <!-- <h2 class="title is-4">Results on DyNeRF dataset</h2> -->
      <!-- <p>
          Here we display results of our MoDGS On DyNeRF dataset. <br><br>
          Change the scene by clicking the button below:
        </p> -->
        <br><br>
        <br><br>
      
 
    <h2 class="title is-3">Apply for 4D Renconstruction</h2>
  </div>
    <!-- <section class="hero is-light is-small"> -->
    <div class="hero-body">
      <!-- <div class="column is-full-width has-text-centered"> -->
      <!-- <div class="container"> -->
        <div class="item item-steve video-grid">
          <div class="video-description0">Ours</div>
          <div class="video-description1">Align3R</div>
          <!-- <div class="video-description2">estimated monocular depth</div>
          <div class="video-description3">rendered depth</div> -->


          <!-- <div class="video-description0">input video</div> -->
          <video poster="" id="steve gt_rgb" autoplay muted loop playsinline width="60%">
            <source src="static/bonn_crowd3.mp4" type="video/mp4">
          </video>
        </div>



      </div>

    </div>
    </div>
  </section>


  <!-- <br>
  <br>
  <br>
  <br>
  <br> -->

      <!-- Abstract -->
  <!-- Abstract -->
  <!-- Abstract -->
  <!-- <section class="section"> -->
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Motion segmentation techniques, which underpin dynamic scene understanding and 4D reconstruction, rely on high-quality spatio-temporal feature modeling to delineate independent motion regions in 3D space. However, existing deep learning methods typically rely on long time-series 2D spatio-temporal information (e.g., appearance information and optical flow) are constrained by the lossy compression inherent in 2D projections and continue to encounter significant performance bottlenecks in complex dynamic scenes characterized by intricate backgrounds or articulated motions.
          </p>
          <p>
            To address these issues, this work proposes a 3D spatio-temporal information priori guided motion segmentation framework aimed at enhancing zero-shot generalization in motion segmentation. The proposed Dual-Dimension Multi-path Information Fusion(D^2MIF) Module employs a recursive multi-path fusion mechanism to synergistically integrate complementary 3D and 2D spatio-temporal information, thereby further improving both the generalization of motion segmentation and fine-grained motion sensing in complex dynamic scenes. Additionally, the framework can be incorporated into the existing 4D reconstruction pipeline as a plug-and-play sub-module that enhances the global geometric consistency of dynamic scene reconstruction through refined motion segmentation masks. We further improve the hierarchical optimization strategy to enhance the usability of the 4D reconstruction pipeline in large-scale dynamic scenes, thereby forming a complete closed loop from the prediction of motion segmentation masks to the 4D reconstruction of dynamic scenes. We conduct extensive cross-domain zero-shot experiments to validate both the robust competitiveness and the generalization performance of the proposed motion segmentation model, and we further demonstrate the superiority of the enhanced 4D reconstruction pipeline on dynamic scene reconstruction datasets.
          </p>
        </div>
      </div>
    </section>
  
    <!-- <section class="section" id="abstract">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column">
            <iframe width="960" height="540" src="https://www.youtube.com/embed/oBN3p716c_k?si=5r9EEMeMLzn6Is1l" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </section> -->

  <section class="section">
    <div class="container is-max-desktop">

      <h2 class="title is-3">Method</h2>
      <div class="container is-max-desktop">
        <img src="static/fig_method.png" alt="Image description" style="display: block; margin: auto;">
        <p>
          <strong>Overview.</strong> Overview of the 3D spatio-temporal information priori-guided motion segmentation framework. 
          We feed time-dependent image pairs into the 4D reconstruction model to extract 3D spatio-temporal priors. 
          The D^2MIF (Dual-Dimension Multi-path Information Fusion) module then integrates 2D and 3D spatio-temporal information 
          from multiple paths and recursively fuses spatio-temporal features to generate dynamic segmentation masks. 
          Additionally, our motion segmentation model is incorporated as a plug-and-play sub-module within the existing 4D scene reconstruction 
          pipeline, thereby enhancing reconstruction quality through the use of refined dynamic mask.
        </p>
      </div>
      <br><br>
      <div class="container is-max-desktop">
        <img src="static/fig_recon_pipeline.png" alt="Image description" style="display: block; margin: auto;">
        <p>
          <strong>Schematic of enhanced 4D renconstruction pipeline.</strong>
          	We construct video graphs from time-continuous video frames using 
          a sliding window approach to form image pairs. For each pair, we predict motion segmentation masks and local pointmaps (top). 
          During batch optimization, an initial globally aligned pointmaps is generated using a Minimum Spanning Tree and rigid registration, 
          followed by iterative optimization via first-order gradient descent (left bottom). 
          Additionally, we optimize the keyframe strategy to enhance global scale consistency in hierarchical optimization (right bottom).
        </p>
      </div>
      <br><br>
    </div>
  </section>





  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This webpage is based on the project page for <a href="https://https://modgs.github.io/">MoDGS</a>, <a href="https://nerf-casting.github.io/">NeRF-Casting</a>,
              <a href="https://camp-nerf.github.io">CamP</a> and <a href="https://nerfies.github.io/">Nerfies</a>. The
              video
              comparison tool is from the <a href="https://dorverbin.github.io/refnerf/index.html">Ref-NeRF</a>
              project.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


</body>

</html>
